{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\textgen\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin d:\\Anaconda3\\envs\\textgen\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    }
   ],
   "source": [
    "from icrl import *\n",
    "from model import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trajectory pkl\n",
    "traj_0 = pickle.load(open('data/linucb_trajectories_part0.pkl', 'rb'))\n",
    "traj_1 = pickle.load(open('data/linucb_trajectories_part1.pkl', 'rb'))\n",
    "traj_2 = pickle.load(open('data/linucb_trajectories_part2.pkl', 'rb'))\n",
    "traj_3 = pickle.load(open('data/linucb_trajectories_part3.pkl', 'rb'))\n",
    "traj_4 = pickle.load(open('data/linucb_trajectories_part4.pkl', 'rb'))\n",
    "\n",
    "# integrate all trajectories\n",
    "traj = []\n",
    "traj.extend(traj_0)\n",
    "traj.extend(traj_1)\n",
    "traj.extend(traj_2)\n",
    "traj.extend(traj_3)\n",
    "traj.extend(traj_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:8yxex5jb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>█▄▃▂▂▁</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>2009.93385</td></tr><tr><td>train_loss</td><td>2068.32754</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">atomic-sun-18</strong> at: <a href='https://wandb.ai/xjxyys/icrl_pretrain/runs/8yxex5jb' target=\"_blank\">https://wandb.ai/xjxyys/icrl_pretrain/runs/8yxex5jb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240626_185243-8yxex5jb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:8yxex5jb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Lenovo\\Desktop\\RA\\Tao Yao\\my_research\\ICRL\\In-context-RL\\wandb\\run-20240626_191206-e24tx3li</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xjxyys/icrl_pretrain/runs/e24tx3li' target=\"_blank\">fancy-hill-19</a></strong> to <a href='https://wandb.ai/xjxyys/icrl_pretrain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xjxyys/icrl_pretrain' target=\"_blank\">https://wandb.ai/xjxyys/icrl_pretrain</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xjxyys/icrl_pretrain/runs/e24tx3li' target=\"_blank\">https://wandb.ai/xjxyys/icrl_pretrain/runs/e24tx3li</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/xjxyys/icrl_pretrain/runs/e24tx3li?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1a017e1cc70>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key = 'da9f2bbc59300d7434138182a83a9b35c0cea793') # login to wandb\n",
    "wandb.init(project=\"icrl_pretrain\", reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "# split trajectory into train and test\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_traj, test_traj = train_test_split(traj, test_size=0.2)\n",
    "\n",
    "batch_size = 16\n",
    "# batch_size = 32\n",
    "\n",
    "# create dataset\n",
    "train_dataset = TrajectoryDataset(train_traj, 10)\n",
    "test_dataset = TrajectoryDataset(test_traj, 10)\n",
    "\n",
    "# create dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# set up model\n",
    "# model = TransformerModel(embed_dim=70, num_heads=5)\n",
    "config = {\n",
    "        'horizon': 200,\n",
    "        'dim':5,\n",
    "        'act_num':10,\n",
    "        'state_dim': 50,\n",
    "        'action_dim': 10,\n",
    "        'n_layer': 8,\n",
    "        'n_embd': 32,\n",
    "        'n_head': 4,\n",
    "        'shuffle': True,\n",
    "        'dropout': 0.2,\n",
    "        'test': False,\n",
    "        'act_type': 'relu',\n",
    "\n",
    "    }\n",
    "action_dim = config['action_dim']\n",
    "model = Transformer(config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device {device}\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "# Trainer\n",
    "def trainer(model,train_dataloader, test_dataloader, optimizer, num_epochs=10):\n",
    "    # DataLoader can handle batching and shuffling\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        epoch_loss = 0.0\n",
    "        for batch in tqdm.tqdm(train_dataloader):\n",
    "            # Unpack the data\n",
    "            # tokens, action_labels = batch\n",
    "            # print(batch['context_actions'].size())\n",
    "            pred_actions = model(batch) # dimension: (batch_size, seq_len, action_dim)\n",
    "            # print(pred_actions.shape)\n",
    "            true_actions = batch['true_actions'] # dimension: (batch_size, seq_len)\n",
    "            pred_actions_flat =  pred_actions.view(-1, action_dim)\n",
    "            true_actions_flat = true_actions.view(-1)\n",
    "            loss = loss_fn(pred_actions_flat, true_actions_flat)\n",
    "            # loss.backward()\n",
    "            # print(action_labels)\n",
    "            # Reset the gradients in the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # # type\n",
    "            # # Forward pass\n",
    "            # logits = model(tokens)  # Your forward method should handle the causal mask internally\n",
    "            \n",
    "            # # Compute loss\n",
    "            # loss = loss(logits, action_labels)\n",
    "    \n",
    "            # # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss /= len(train_dataloader)\n",
    "        # Compute the test loss\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm.tqdm(test_dataloader):\n",
    "                pred_actions = model(batch)\n",
    "                true_actions = batch['true_actions']\n",
    "                pred_actions_flat =  pred_actions.view(-1, action_dim)\n",
    "                true_actions_flat = true_actions.view(-1)\n",
    "                loss = loss_fn(pred_actions_flat, true_actions_flat)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "        model.train()\n",
    "        test_loss /= len(test_dataloader)\n",
    "        # wandb logging\n",
    "        wandb.log({\"train_loss\": epoch_loss, \"test_loss\": test_loss})\n",
    "        print(f\"Epoch {epoch} | Train Loss {epoch_loss} | Test Loss {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:30<00:00, 23.75it/s]\n",
      "100%|██████████| 1250/1250 [00:34<00:00, 35.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss 2924.667772167969 | Test Loss 1246.77803984375\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:34<00:00, 23.30it/s]\n",
      "100%|██████████| 1250/1250 [00:35<00:00, 35.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss 1181.0856947021484 | Test Loss 1125.112143408203\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:32<00:00, 23.55it/s]\n",
      "100%|██████████| 1250/1250 [00:35<00:00, 35.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss 1096.900336730957 | Test Loss 1087.7575541992187\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:33<00:00, 23.42it/s]\n",
      "100%|██████████| 1250/1250 [00:34<00:00, 36.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss 1052.7781550415039 | Test Loss 1038.267660546875\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:12<00:00, 25.94it/s]\n",
      "100%|██████████| 1250/1250 [00:31<00:00, 39.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss 1030.9833446655273 | Test Loss 1012.7422104003906\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:13<00:00, 25.82it/s]\n",
      "100%|██████████| 1250/1250 [00:30<00:00, 40.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss 1004.6154026245117 | Test Loss 995.4693988769532\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:08<00:00, 26.47it/s]\n",
      "100%|██████████| 1250/1250 [00:30<00:00, 40.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss 969.9048502929687 | Test Loss 947.2046483886719\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:12<00:00, 25.93it/s]\n",
      "100%|██████████| 1250/1250 [00:31<00:00, 40.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss 939.7687962036133 | Test Loss 927.7218052734376\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:11<00:00, 26.13it/s]\n",
      "100%|██████████| 1250/1250 [00:32<00:00, 38.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss 916.8840484863281 | Test Loss 915.9941775878906\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:15<00:00, 25.56it/s]\n",
      "100%|██████████| 1250/1250 [00:31<00:00, 39.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss 900.483264465332 | Test Loss 896.4743614257812\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:14<00:00, 25.69it/s]\n",
      "100%|██████████| 1250/1250 [00:31<00:00, 39.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss 884.6308341064453 | Test Loss 880.5619122558594\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:12<00:00, 25.99it/s]\n",
      "100%|██████████| 1250/1250 [00:31<00:00, 39.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss 871.2012689941406 | Test Loss 863.25072421875\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:12<00:00, 26.04it/s]\n",
      "100%|██████████| 1250/1250 [00:31<00:00, 40.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss 859.4235528686523 | Test Loss 855.276259765625\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:12<00:00, 25.99it/s]\n",
      "100%|██████████| 1250/1250 [00:31<00:00, 39.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss 851.4043709594727 | Test Loss 845.0645205566407\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:11<00:00, 26.05it/s]\n",
      "100%|██████████| 1250/1250 [00:31<00:00, 39.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss 844.6609286987305 | Test Loss 836.3830283203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer(model, train_dataloader, test_dataloader, optimizer, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), 'model/transformer_model_0626.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomChoice + Qtransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxjxyys\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Lenovo/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\RA\\Tao Yao\\my_research\\ICRL\\In-context-RL\\wandb\\run-20240705_144327-0ikmjgnm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xjxyys/icrl_pretrain/runs/0ikmjgnm' target=\"_blank\">fancy-puddle-30</a></strong> to <a href='https://wandb.ai/xjxyys/icrl_pretrain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xjxyys/icrl_pretrain' target=\"_blank\">https://wandb.ai/xjxyys/icrl_pretrain</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xjxyys/icrl_pretrain/runs/0ikmjgnm' target=\"_blank\">https://wandb.ai/xjxyys/icrl_pretrain/runs/0ikmjgnm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/xjxyys/icrl_pretrain/runs/0ikmjgnm?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1d830f48c40>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key = 'da9f2bbc59300d7434138182a83a9b35c0cea793') # login to wandb\n",
    "wandb.init(project=\"icrl_pretrain\", reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load trajectory pkl\n",
    "traj_0 = pickle.load(open('data/RandomChoose_trajectories_part0.pkl', 'rb'))\n",
    "traj_1 = pickle.load(open('data/RandomChoose_trajectories_part1.pkl', 'rb'))\n",
    "traj_2 = pickle.load(open('data/RandomChoose_trajectories_part2.pkl', 'rb'))\n",
    "traj_3 = pickle.load(open('data/RandomChoose_trajectories_part3.pkl', 'rb'))\n",
    "traj_4 = pickle.load(open('data/RandomChoose_trajectories_part4.pkl', 'rb'))\n",
    "# integrate all trajectories\n",
    "traj = []\n",
    "traj.extend(traj_0)\n",
    "traj.extend(traj_1)\n",
    "traj.extend(traj_2)\n",
    "traj.extend(traj_3)\n",
    "traj.extend(traj_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split trajectory into train and test\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from icrl import *\n",
    "\n",
    "train_traj, test_traj = train_test_split(traj, test_size=0.2)\n",
    "\n",
    "# batch_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "# create dataset\n",
    "train_dataset = TrajectoryDataset(train_traj, 10)\n",
    "test_dataset = TrajectoryDataset(test_traj, 10)\n",
    "\n",
    "# create dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "from model import *\n",
    "config = {\n",
    "        'horizon': 200,\n",
    "        'dim':5,\n",
    "        'act_num':10,\n",
    "        'state_dim': 50,\n",
    "        'action_dim': 10,\n",
    "        'n_layer': 8,\n",
    "        'n_embd': 32,\n",
    "        'n_head': 4,\n",
    "        'shuffle': True,\n",
    "        'dropout': 0.2,\n",
    "        'test': False,\n",
    "        'act_type': 'relu',\n",
    "        'pred_q': True\n",
    "    }\n",
    "# train\n",
    "action_dim = config['action_dim']\n",
    "\n",
    "model = Transformer(config)\n",
    "# copy the model\n",
    "# target_model = Transformer(config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device {device}\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2.5e-4, weight_decay=1e-4)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), l=1e-5, weight_decay=1e-4)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=1e-4)\n",
    "# loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "loss_fn = F.mse_loss\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [01:28<00:00, 28.30it/s]\n",
      "100%|██████████| 625/625 [00:22<00:00, 27.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 4.942751865005493 | Test Loss 4.947106595611572\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [03:54<00:00, 10.66it/s]\n",
      "100%|██████████| 625/625 [00:46<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss 2.8417072288513183 | Test Loss 3.452147011947632\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [04:25<00:00,  9.42it/s]\n",
      "100%|██████████| 625/625 [00:46<00:00, 13.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss 2.797916157436371 | Test Loss 2.7928110874176024\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [04:26<00:00,  9.40it/s]\n",
      "100%|██████████| 625/625 [00:47<00:00, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss 2.6036930936813354 | Test Loss 2.69954414024353\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [04:24<00:00,  9.47it/s]\n",
      "100%|██████████| 625/625 [00:48<00:00, 13.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss 2.6587734623909 | Test Loss 2.6575336460113523\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [04:17<00:00,  9.69it/s]\n",
      "100%|██████████| 625/625 [00:46<00:00, 13.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss 2.71507711725235 | Test Loss 2.6220011482238768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Q_trainer(model, train_dataloader, test_dataloader, optimizer, num_epochs=5, double=True, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), 'model/Qtransformer_model_0706_2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# randomChoose + Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# load trajectory pkl\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m traj_0 \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/RandomChoose_trajectories_part0.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m traj_1 \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/RandomChoose_trajectories_part1.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      5\u001b[0m traj_2 \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/RandomChoose_trajectories_part2.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# load trajectory pkl\n",
    "traj_0 = pickle.load(open('data/RandomChoose_trajectories_part0.pkl', 'rb'))\n",
    "traj_1 = pickle.load(open('data/RandomChoose_trajectories_part1.pkl', 'rb'))\n",
    "traj_2 = pickle.load(open('data/RandomChoose_trajectories_part2.pkl', 'rb'))\n",
    "traj_3 = pickle.load(open('data/RandomChoose_trajectories_part3.pkl', 'rb'))\n",
    "traj_4 = pickle.load(open('data/RandomChoose_trajectories_part4.pkl', 'rb'))\n",
    "# integrate all trajectories\n",
    "traj = []\n",
    "traj.extend(traj_0)\n",
    "traj.extend(traj_1)\n",
    "traj.extend(traj_2)\n",
    "traj.extend(traj_3)\n",
    "traj.extend(traj_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "# split trajectory into train and test\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_traj, test_traj = train_test_split(traj, test_size=0.2)\n",
    "\n",
    "batch_size = 16\n",
    "# batch_size = 32\n",
    "\n",
    "# create dataset\n",
    "train_dataset = TrajectoryDataset(train_traj, 10)\n",
    "test_dataset = TrajectoryDataset(test_traj, 10)\n",
    "\n",
    "# create dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# set up model\n",
    "# model = TransformerModel(embed_dim=70, num_heads=5)\n",
    "config = {\n",
    "        'horizon': 200,\n",
    "        'dim':5,\n",
    "        'act_num':10,\n",
    "        'state_dim': 50,\n",
    "        'action_dim': 10,\n",
    "        'n_layer': 8,\n",
    "        'n_embd': 32,\n",
    "        'n_head': 4,\n",
    "        'shuffle': True,\n",
    "        'dropout': 0.2,\n",
    "        'test': False,\n",
    "        'act_type': 'relu',\n",
    "        'pred_q': False\n",
    "        \n",
    "\n",
    "    }\n",
    "action_dim = config['action_dim']\n",
    "model = Transformer(config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device {device}\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:39<00:00, 22.73it/s]\n",
      "100%|██████████| 1250/1250 [00:38<00:00, 32.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss 7369.991474316406 | Test Loss 7369.60325234375\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:36<00:00, 23.05it/s]\n",
      "100%|██████████| 1250/1250 [00:36<00:00, 34.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss 7368.959986425782 | Test Loss 7369.124048828125\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:38<00:00, 22.84it/s]\n",
      "100%|██████████| 1250/1250 [00:35<00:00, 34.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss 7368.804815234375 | Test Loss 7368.98579140625\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:35<00:00, 23.24it/s]\n",
      "100%|██████████| 1250/1250 [00:31<00:00, 39.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss 7368.713823730469 | Test Loss 7368.627779296875\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:21<00:00, 24.78it/s]\n",
      "100%|██████████| 1250/1250 [00:32<00:00, 38.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss 7368.6082125 | Test Loss 7368.526080078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer(model, train_dataloader, test_dataloader, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), 'model/transformer_model_random.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinUCB + Qtransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trajectory pkl\n",
    "import pickle\n",
    "traj_0 = pickle.load(open('data/linucb_trajectories_part0.pkl', 'rb'))\n",
    "traj_1 = pickle.load(open('data/linucb_trajectories_part1.pkl', 'rb'))\n",
    "traj_2 = pickle.load(open('data/linucb_trajectories_part2.pkl', 'rb'))\n",
    "traj_3 = pickle.load(open('data/linucb_trajectories_part3.pkl', 'rb'))\n",
    "traj_4 = pickle.load(open('data/linucb_trajectories_part4.pkl', 'rb'))\n",
    "\n",
    "# integrate all trajectories\n",
    "traj = []\n",
    "traj.extend(traj_0)\n",
    "traj.extend(traj_1)\n",
    "traj.extend(traj_2)\n",
    "traj.extend(traj_3)\n",
    "traj.extend(traj_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\textgen\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin d:\\Anaconda3\\envs\\textgen\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    }
   ],
   "source": [
    "# split trajectory into train and test\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from icrl import *\n",
    "\n",
    "train_traj, test_traj = train_test_split(traj, test_size=0.2)\n",
    "\n",
    "batch_size = 16\n",
    "# batch_size = 32\n",
    "\n",
    "# create dataset\n",
    "train_dataset = TrajectoryDataset(train_traj, 10)\n",
    "test_dataset = TrajectoryDataset(test_traj, 10)\n",
    "\n",
    "# create dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxjxyys\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Lenovo/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\RA\\Tao Yao\\my_research\\ICRL\\In-context-RL\\wandb\\run-20240707_160958-bog6gerl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xjxyys/icrl_pretrain/runs/bog6gerl' target=\"_blank\">happy-deluge-32</a></strong> to <a href='https://wandb.ai/xjxyys/icrl_pretrain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xjxyys/icrl_pretrain' target=\"_blank\">https://wandb.ai/xjxyys/icrl_pretrain</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xjxyys/icrl_pretrain/runs/bog6gerl' target=\"_blank\">https://wandb.ai/xjxyys/icrl_pretrain/runs/bog6gerl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/xjxyys/icrl_pretrain/runs/bog6gerl?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x257791be980>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key = 'da9f2bbc59300d7434138182a83a9b35c0cea793') # login to wandb\n",
    "wandb.init(project=\"icrl_pretrain\", reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "config = {\n",
    "        'horizon': 200,\n",
    "        'dim':5,\n",
    "        'act_num':10,\n",
    "        'state_dim': 50,\n",
    "        'action_dim': 10,\n",
    "        'n_layer': 8,\n",
    "        'n_embd': 32,\n",
    "        'n_head': 4,\n",
    "        'shuffle': True,\n",
    "        'dropout': 0.2,\n",
    "        'test': False,\n",
    "        'act_type': 'relu',\n",
    "        'pred_q': True\n",
    "    }\n",
    "# train\n",
    "action_dim = config['action_dim']\n",
    "model = Transformer(config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-4)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "loss_fn = F.mse_loss\n",
    "gamma = 1\n",
    "# Trainer\n",
    "def Q_trainer(model, train_dataloader, test_dataloader, optimizer, num_epochs=10, double=False):\n",
    "    # record the loss before training\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(train_dataloader):\n",
    "            Qvalues = model(batch) # dimension: (batch_size, seq_len, A)\n",
    "            # pred_Qvalues = pred_Qvalues.gather(2, batch['context_actions'].unsqueeze(-1).long()) # get the Q value of the context actions\n",
    "            next_Qvalues = Qvalues.max(dim=-1, keepdim=True)[0]\n",
    "            next_Qvalues = torch.cat([next_Qvalues[:, 1:,:], torch.zeros((next_Qvalues.shape[0], 1, 1), device=next_Qvalues.device)], dim=1)\n",
    "            pred_Qvalues =  Qvalues * batch['context_actions']\n",
    "            pred_Qvalues =  pred_Qvalues.sum(dim=-1, keepdim=True)\n",
    "            TD_target = batch['context_rewards'] + gamma * next_Qvalues\n",
    "            loss = loss_fn(pred_Qvalues, TD_target)\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_dataloader)\n",
    "\n",
    "        for batch in tqdm.tqdm(test_dataloader):\n",
    "            # pred_Qvalues = model(batch).gather(2, batch['context_actions'].unsqueeze(-1).long())\n",
    "            Qvalues = model(batch)\n",
    "            next_Qvalues = Qvalues.max(dim=-1, keepdim=True)[0]\n",
    "\n",
    "            next_Qvalues = torch.cat([next_Qvalues[:, 1:, :], torch.zeros((next_Qvalues.shape[0], 1, 1), device=next_Qvalues.device)], dim=1)\n",
    "            pred_Qvalues =  Qvalues * batch['context_actions']\n",
    "            pred_Qvalues =  pred_Qvalues.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        \n",
    "            # shifted_pred_Qvalues = torch.cat([pred_Qvalues[:, 1:, :], torch.zeros((pred_Qvalues.shape[0], 1, 1), device=pred_Qvalues.device)], dim=1)\n",
    "            TD_target = batch['context_rewards'] + gamma * next_Qvalues\n",
    "            loss = loss_fn(pred_Qvalues, TD_target)\n",
    "            test_loss += loss.item()\n",
    "        test_loss /= len(test_dataloader)\n",
    "        print(f\"Train Loss {train_loss} | Test Loss {test_loss}\")\n",
    "        wandb.log({\"train_loss\": train_loss, \"test_loss\": test_loss})\n",
    "    # DataLoader can handle batching and shuffling\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    if double:\n",
    "        target_model = Transformer(config)\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "        target_model.eval()\n",
    "        target_model.to(device)\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        epoch_loss = 0.0\n",
    "        counter = 0\n",
    "        for batch in tqdm.tqdm(train_dataloader):\n",
    "            Qvalues = model(batch) # dimension: (batch_size, seq_len, A)\n",
    "            pred_Qvalues = Qvalues * batch['context_actions']\n",
    "            pred_Qvalues =  pred_Qvalues.sum(dim=-1, keepdim=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                next_actions = model(batch).max(dim=-1, keepdim=True)[1]\n",
    "\n",
    "                if double:\n",
    "                    # use target model to get the next Q values\n",
    "                    # next_Qvalues = target_model(batch).gather(2, next_actions.unsqueeze(-1).long())\n",
    "                    next_Qvalues = target_model(batch).gather(2, next_actions.long())\n",
    "                else:\n",
    "                    next_Qvalues = Qvalues.max(dim=-1, keepdim=True)[0]\n",
    "                next_Qvalues = torch.cat([next_Qvalues[:, 1:, :], torch.zeros((next_Qvalues.shape[0], 1, 1), device=next_Qvalues.device)], dim=1)\n",
    "            # Note: batch['context_rewards'] should be in the shape (batch_size, seq_len, 1) to match pred_Qvalues\n",
    "            TD_target = batch['context_rewards'] + gamma * next_Qvalues\n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(pred_Qvalues, TD_target, reduction='mean')\n",
    "            # Reset the gradients in the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # # Backward pass\n",
    "            loss.backward()\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            counter += 1\n",
    "            # if double:\n",
    "            #     for param, target_param in zip(model.parameters(), target_model.parameters()):\n",
    "            #         target_param.data = target_param.data * (1 - 0.01) + param.data * 0.01\n",
    "            if double:\n",
    "                if counter % 100 == 0:\n",
    "                    target_model.load_state_dict(model.state_dict())\n",
    "            \n",
    "        epoch_loss /= len(train_dataloader)\n",
    "        # Compute the test loss\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm.tqdm(test_dataloader):\n",
    "                # pred_Qvalues = model(batch).gather(2, batch['context_actions'].unsqueeze(-1).long())\n",
    "                Qvalues = model(batch)\n",
    "                pred_Qvalues = Qvalues * batch['context_actions']\n",
    "                pred_Qvalues =  pred_Qvalues.sum(dim=-1, keepdim=True)\n",
    "                next_actions = model(batch).max(dim=-1, keepdim=True)[1]\n",
    "                if double:\n",
    "    \n",
    "                    next_Qvalues = target_model(batch).gather(2, next_actions.long())\n",
    "                else:\n",
    "                    next_Qvalues = Qvalues.max(dim=-1, keepdim=True)[0]\n",
    "                    \n",
    "                next_Qvalues = torch.cat([next_Qvalues[:, 1:, :], torch.zeros((next_Qvalues.shape[0], 1, 1), device=next_Qvalues.device)], dim=1)\n",
    "                # shifted_pred_Qvalues = torch.cat([pred_Qvalues[:, 1:, :], torch.zeros((pred_Qvalues.shape[0], 1, 1), device=pred_Qvalues.device)], dim=1)\n",
    "                \n",
    "                TD_target = batch['context_rewards'] + gamma * next_Qvalues\n",
    "\n",
    "                loss = loss_fn(pred_Qvalues, TD_target)\n",
    "                test_loss += loss.item()\n",
    "        model.train()\n",
    "        test_loss /= len(test_dataloader)\n",
    "        # wandb logging\n",
    "        wandb.log({\"train_loss\": epoch_loss, \"test_loss\": test_loss})\n",
    "        print(f\"Epoch {epoch} | Train Loss {epoch_loss} | Test Loss {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [02:49<00:00, 29.44it/s]\n",
      "100%|██████████| 1250/1250 [00:44<00:00, 28.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 6.694190567207336 | Test Loss 6.6788156131744385\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [05:20<00:00, 15.60it/s]\n",
      "100%|██████████| 1250/1250 [01:01<00:00, 20.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss 4.526185640001297 | Test Loss 4.9170932235717775\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [05:33<00:00, 14.99it/s]\n",
      "100%|██████████| 1250/1250 [01:04<00:00, 19.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss 5.578145941114426 | Test Loss 7.751960882949829\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [05:25<00:00, 15.35it/s]\n",
      "100%|██████████| 1250/1250 [00:58<00:00, 21.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss 9.400312283039092 | Test Loss 4.173528666305542\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [05:21<00:00, 15.54it/s]\n",
      "100%|██████████| 1250/1250 [00:59<00:00, 20.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss 3.4564990319252016 | Test Loss 3.602992325973511\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [05:14<00:00, 15.92it/s]\n",
      "100%|██████████| 1250/1250 [00:57<00:00, 21.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss 3.3751141761779784 | Test Loss 3.736875250053406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Q_trainer(model, train_dataloader, test_dataloader, optimizer, num_epochs=5, double=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), 'model/Qtransformer_model_ucb_0707.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
