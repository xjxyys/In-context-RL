{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\textgen\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin d:\\Anaconda3\\envs\\textgen\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    }
   ],
   "source": [
    "from icrl import *\n",
    "from model import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trajectory pkl\n",
    "traj_0 = pickle.load(open('data/linucb_trajectories_part0.pkl', 'rb'))\n",
    "traj_1 = pickle.load(open('data/linucb_trajectories_part1.pkl', 'rb'))\n",
    "traj_2 = pickle.load(open('data/linucb_trajectories_part2.pkl', 'rb'))\n",
    "traj_3 = pickle.load(open('data/linucb_trajectories_part3.pkl', 'rb'))\n",
    "traj_4 = pickle.load(open('data/linucb_trajectories_part4.pkl', 'rb'))\n",
    "\n",
    "# integrate all trajectories\n",
    "traj = []\n",
    "traj.extend(traj_0)\n",
    "traj.extend(traj_1)\n",
    "traj.extend(traj_2)\n",
    "traj.extend(traj_3)\n",
    "traj.extend(traj_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:8yxex5jb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>█▄▃▂▂▁</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>2009.93385</td></tr><tr><td>train_loss</td><td>2068.32754</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">atomic-sun-18</strong> at: <a href='https://wandb.ai/xjxyys/icrl_pretrain/runs/8yxex5jb' target=\"_blank\">https://wandb.ai/xjxyys/icrl_pretrain/runs/8yxex5jb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240626_185243-8yxex5jb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:8yxex5jb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Lenovo\\Desktop\\RA\\Tao Yao\\my_research\\ICRL\\In-context-RL\\wandb\\run-20240626_191206-e24tx3li</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xjxyys/icrl_pretrain/runs/e24tx3li' target=\"_blank\">fancy-hill-19</a></strong> to <a href='https://wandb.ai/xjxyys/icrl_pretrain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xjxyys/icrl_pretrain' target=\"_blank\">https://wandb.ai/xjxyys/icrl_pretrain</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xjxyys/icrl_pretrain/runs/e24tx3li' target=\"_blank\">https://wandb.ai/xjxyys/icrl_pretrain/runs/e24tx3li</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/xjxyys/icrl_pretrain/runs/e24tx3li?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1a017e1cc70>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key = 'da9f2bbc59300d7434138182a83a9b35c0cea793') # login to wandb\n",
    "wandb.init(project=\"icrl_pretrain\", reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "# split trajectory into train and test\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_traj, test_traj = train_test_split(traj, test_size=0.2)\n",
    "\n",
    "batch_size = 16\n",
    "# batch_size = 32\n",
    "\n",
    "# create dataset\n",
    "train_dataset = TrajectoryDataset(train_traj, 10)\n",
    "test_dataset = TrajectoryDataset(test_traj, 10)\n",
    "\n",
    "# create dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# set up model\n",
    "# model = TransformerModel(embed_dim=70, num_heads=5)\n",
    "config = {\n",
    "        'horizon': 200,\n",
    "        'dim':5,\n",
    "        'act_num':10,\n",
    "        'state_dim': 50,\n",
    "        'action_dim': 10,\n",
    "        'n_layer': 8,\n",
    "        'n_embd': 32,\n",
    "        'n_head': 4,\n",
    "        'shuffle': True,\n",
    "        'dropout': 0.2,\n",
    "        'test': False,\n",
    "        'act_type': 'relu',\n",
    "    }\n",
    "action_dim = config['action_dim']\n",
    "model = Transformer(config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device {device}\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "# Trainer\n",
    "def trainer(model,train_dataloader, test_dataloader, optimizer, num_epochs=10):\n",
    "    # DataLoader can handle batching and shuffling\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        epoch_loss = 0.0\n",
    "        for batch in tqdm.tqdm(train_dataloader):\n",
    "            # Unpack the data\n",
    "            # tokens, action_labels = batch\n",
    "            # print(batch['context_actions'].size())\n",
    "            pred_actions = model(batch) # dimension: (batch_size, seq_len, action_dim)\n",
    "            # print(pred_actions.shape)\n",
    "            true_actions = batch['true_actions'] # dimension: (batch_size, seq_len)\n",
    "            pred_actions_flat =  pred_actions.view(-1, action_dim)\n",
    "            true_actions_flat = true_actions.view(-1)\n",
    "            loss = loss_fn(pred_actions_flat, true_actions_flat)\n",
    "            # loss.backward()\n",
    "            # print(action_labels)\n",
    "            # Reset the gradients in the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # # type\n",
    "            # # Forward pass\n",
    "            # logits = model(tokens)  # Your forward method should handle the causal mask internally\n",
    "            \n",
    "            # # Compute loss\n",
    "            # loss = loss(logits, action_labels)\n",
    "    \n",
    "            # # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss /= len(train_dataloader)\n",
    "        # Compute the test loss\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm.tqdm(test_dataloader):\n",
    "                pred_actions = model(batch)\n",
    "                true_actions = batch['true_actions']\n",
    "                pred_actions_flat =  pred_actions.view(-1, action_dim)\n",
    "                true_actions_flat = true_actions.view(-1)\n",
    "                loss = loss_fn(pred_actions_flat, true_actions_flat)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "        model.train()\n",
    "        test_loss /= len(test_dataloader)\n",
    "        # wandb logging\n",
    "        wandb.log({\"train_loss\": epoch_loss, \"test_loss\": test_loss})\n",
    "        print(f\"Epoch {epoch} | Train Loss {epoch_loss} | Test Loss {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:30<00:00, 23.75it/s]\n",
      "100%|██████████| 1250/1250 [00:34<00:00, 35.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss 2924.667772167969 | Test Loss 1246.77803984375\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:34<00:00, 23.30it/s]\n",
      "100%|██████████| 1250/1250 [00:35<00:00, 35.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss 1181.0856947021484 | Test Loss 1125.112143408203\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:32<00:00, 23.55it/s]\n",
      "100%|██████████| 1250/1250 [00:35<00:00, 35.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss 1096.900336730957 | Test Loss 1087.7575541992187\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:33<00:00, 23.42it/s]\n",
      "100%|██████████| 1250/1250 [00:34<00:00, 36.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss 1052.7781550415039 | Test Loss 1038.267660546875\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:12<00:00, 25.94it/s]\n",
      "100%|██████████| 1250/1250 [00:31<00:00, 39.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss 1030.9833446655273 | Test Loss 1012.7422104003906\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:13<00:00, 25.82it/s]\n",
      "100%|██████████| 1250/1250 [00:30<00:00, 40.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss 1004.6154026245117 | Test Loss 995.4693988769532\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:08<00:00, 26.47it/s]\n",
      "100%|██████████| 1250/1250 [00:30<00:00, 40.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss 969.9048502929687 | Test Loss 947.2046483886719\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:12<00:00, 25.93it/s]\n",
      "100%|██████████| 1250/1250 [00:31<00:00, 40.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss 939.7687962036133 | Test Loss 927.7218052734376\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:11<00:00, 26.13it/s]\n",
      "100%|██████████| 1250/1250 [00:32<00:00, 38.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss 916.8840484863281 | Test Loss 915.9941775878906\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:15<00:00, 25.56it/s]\n",
      "100%|██████████| 1250/1250 [00:31<00:00, 39.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss 900.483264465332 | Test Loss 896.4743614257812\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:14<00:00, 25.69it/s]\n",
      "100%|██████████| 1250/1250 [00:31<00:00, 39.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss 884.6308341064453 | Test Loss 880.5619122558594\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:12<00:00, 25.99it/s]\n",
      "100%|██████████| 1250/1250 [00:31<00:00, 39.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss 871.2012689941406 | Test Loss 863.25072421875\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:12<00:00, 26.04it/s]\n",
      "100%|██████████| 1250/1250 [00:31<00:00, 40.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss 859.4235528686523 | Test Loss 855.276259765625\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:12<00:00, 25.99it/s]\n",
      "100%|██████████| 1250/1250 [00:31<00:00, 39.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss 851.4043709594727 | Test Loss 845.0645205566407\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:11<00:00, 26.05it/s]\n",
      "100%|██████████| 1250/1250 [00:31<00:00, 39.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss 844.6609286987305 | Test Loss 836.3830283203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer(model, train_dataloader, test_dataloader, optimizer, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), 'model/transformer_model_0626.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomChoice + Qtransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxjxyys\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Lenovo/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\RA\\Tao Yao\\my_research\\ICRL\\In-context-RL\\wandb\\run-20240717_152104-kwv32c63</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xjxyys/icrl_pretrain/runs/kwv32c63' target=\"_blank\">gentle-voice-46</a></strong> to <a href='https://wandb.ai/xjxyys/icrl_pretrain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xjxyys/icrl_pretrain' target=\"_blank\">https://wandb.ai/xjxyys/icrl_pretrain</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xjxyys/icrl_pretrain/runs/kwv32c63' target=\"_blank\">https://wandb.ai/xjxyys/icrl_pretrain/runs/kwv32c63</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/xjxyys/icrl_pretrain/runs/kwv32c63?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x25a831043a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key = 'da9f2bbc59300d7434138182a83a9b35c0cea793') # login to wandb\n",
    "wandb.init(project=\"icrl_pretrain\", reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load trajectory pkl\n",
    "traj_0 = pickle.load(open('data/RandomChoose_trajectories_part0.pkl', 'rb'))\n",
    "traj_1 = pickle.load(open('data/RandomChoose_trajectories_part1.pkl', 'rb'))\n",
    "traj_2 = pickle.load(open('data/RandomChoose_trajectories_part2.pkl', 'rb'))\n",
    "traj_3 = pickle.load(open('data/RandomChoose_trajectories_part3.pkl', 'rb'))\n",
    "traj_4 = pickle.load(open('data/RandomChoose_trajectories_part4.pkl', 'rb'))\n",
    "# integrate all trajectories\n",
    "traj = []\n",
    "traj.extend(traj_0)\n",
    "traj.extend(traj_1)\n",
    "traj.extend(traj_2)\n",
    "traj.extend(traj_3)\n",
    "traj.extend(traj_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\textgen\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin d:\\Anaconda3\\envs\\textgen\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    }
   ],
   "source": [
    "# split trajectory into train and test\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from icrl import *\n",
    "\n",
    "train_traj, test_traj = train_test_split(traj, test_size=0.2)\n",
    "\n",
    "# batch_size = 64\n",
    "batch_size = 32\n",
    "# batch_size = 16\n",
    "\n",
    "# create dataset\n",
    "train_dataset = TrajectoryDataset(train_traj, 10)\n",
    "test_dataset = TrajectoryDataset(test_traj, 10)\n",
    "\n",
    "# create dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "from model import *\n",
    "from icrl import *\n",
    "config = {\n",
    "        'horizon': 200,\n",
    "        'dim':5,\n",
    "        'act_num':10,\n",
    "        'state_dim': 50,\n",
    "        'action_dim': 10,\n",
    "        'n_layer': 8,\n",
    "        'n_embd': 32,\n",
    "        'n_head': 4,\n",
    "        'shuffle': True,\n",
    "        'dropout': 0.2,\n",
    "        'test': False,\n",
    "        'act_type': 'relu',\n",
    "        'pred_q': True\n",
    "    }\n",
    "# train\n",
    "action_dim = config['action_dim']\n",
    "\n",
    "model = Transformer(config)\n",
    "# copy the model\n",
    "# target_model = Transformer(config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=2.5e-4, weight_decay=1e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=7.5e-6, weight_decay=1e-4)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=1e-4)\n",
    "# loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "loss_fn = F.mse_loss\n",
    "gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_trainer(model, train_dataloader, test_dataloader, optimizer, loss_fn, num_epochs=10, gamma = 0.99, double=False):\n",
    "    # record the loss before training\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(train_dataloader):\n",
    "            Qvalues = model(batch) # dimension: (batch_size, seq_len, A)\n",
    "            # pred_Qvalues = pred_Qvalues.gather(2, batch['context_actions'].unsqueeze(-1).long()) # get the Q value of the context actions\n",
    "            next_Qvalues = Qvalues.max(dim=-1, keepdim=True)[0]\n",
    "            expQ = torch.exp(Qvalues)\n",
    "            sumexpQ = expQ.sum(dim=-1, keepdim=True)\n",
    "            next_Qvalues = (expQ/sumexpQ * Qvalues).sum(dim=-1, keepdim=True)\n",
    "            \n",
    "            next_Qvalues = torch.cat([next_Qvalues[:, 1:,:], torch.zeros((next_Qvalues.shape[0], 1, 1), device=next_Qvalues.device)], dim=1)\n",
    "            pred_Qvalues =  Qvalues * batch['context_actions']\n",
    "            pred_Qvalues =  pred_Qvalues.sum(dim=-1, keepdim=True)\n",
    "            \n",
    "            TD_target = batch['context_rewards'] + gamma * next_Qvalues\n",
    "            loss = loss_fn(pred_Qvalues, TD_target)\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_dataloader)\n",
    "\n",
    "        for batch in tqdm.tqdm(test_dataloader):\n",
    "            # pred_Qvalues = model(batch).gather(2, batch['context_actions'].unsqueeze(-1).long())\n",
    "            Qvalues = model(batch)\n",
    "            next_Qvalues = Qvalues.max(dim=-1, keepdim=True)[0]\n",
    "\n",
    "            next_Qvalues = torch.cat([next_Qvalues[:, 1:, :], torch.zeros((next_Qvalues.shape[0], 1, 1), device=next_Qvalues.device)], dim=1)\n",
    "            pred_Qvalues =  Qvalues * batch['context_actions']\n",
    "            pred_Qvalues =  pred_Qvalues.sum(dim=-1, keepdim=True)\n",
    "            \n",
    "            TD_target = batch['context_rewards'] + gamma * next_Qvalues\n",
    "            loss = loss_fn(pred_Qvalues, TD_target)\n",
    "            test_loss += loss.item()\n",
    "        test_loss /= len(test_dataloader)\n",
    "        print(f\"Train Loss {train_loss} | Test Loss {test_loss}\")\n",
    "        wandb.log({\"train_loss\": train_loss, \"test_loss\": test_loss})\n",
    "    # DataLoader can handle batching and shuffling\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    if double:\n",
    "        global config\n",
    "        target_model = Transformer(config)\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "        target_model.eval()\n",
    "        target_model.to(device)\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        epoch_loss = 0.0\n",
    "        counter = 0\n",
    "        for batch in tqdm.tqdm(train_dataloader):\n",
    "            Qvalues = model(batch) # dimension: (batch_size, seq_len, A)\n",
    "            pred_Qvalues = Qvalues * batch['context_actions']\n",
    "            pred_Qvalues =  pred_Qvalues.sum(dim=-1, keepdim=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                next_actions = model(batch).max(dim=-1, keepdim=True)[1]\n",
    "\n",
    "                if double:\n",
    "                    # use target model to get the next Q values\n",
    "                    # next_Qvalues = target_model(batch).gather(2, next_actions.unsqueeze(-1).long())\n",
    "                    next_Qvalues = target_model(batch).gather(2, next_actions.long())\n",
    "                else:\n",
    "                    next_Qvalues = Qvalues.max(dim=-1, keepdim=True)[0]\n",
    "                next_Qvalues = torch.cat([next_Qvalues[:, 1:, :], torch.zeros((next_Qvalues.shape[0], 1, 1), device=next_Qvalues.device)], dim=1)\n",
    "            # Note: batch['context_rewards'] should be in the shape (batch_size, seq_len, 1) to match pred_Qvalues\n",
    "            TD_target = batch['context_rewards'] + gamma * next_Qvalues\n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(pred_Qvalues, TD_target, reduction='mean')\n",
    "            # Reset the gradients in the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # # Backward pass\n",
    "            loss.backward()\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            counter += 1\n",
    "            # if double:\n",
    "            #     for param, target_param in zip(model.parameters(), target_model.parameters()):\n",
    "            #         target_param.data = target_param.data * (1 - 0.01) + param.data * 0.01\n",
    "            if double:\n",
    "                if counter % 100 == 0:\n",
    "                    target_model.load_state_dict(model.state_dict())\n",
    "            \n",
    "        epoch_loss /= len(train_dataloader)\n",
    "        # Compute the test loss\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm.tqdm(test_dataloader):\n",
    "                # pred_Qvalues = model(batch).gather(2, batch['context_actions'].unsqueeze(-1).long())\n",
    "                Qvalues = model(batch)\n",
    "                pred_Qvalues = Qvalues * batch['context_actions']\n",
    "                pred_Qvalues =  pred_Qvalues.sum(dim=-1, keepdim=True)\n",
    "                next_actions = model(batch).max(dim=-1, keepdim=True)[1]\n",
    "                if double:\n",
    "    \n",
    "                    next_Qvalues = target_model(batch).gather(2, next_actions.long())\n",
    "                else:\n",
    "                    next_Qvalues = Qvalues.max(dim=-1, keepdim=True)[0]\n",
    "                    \n",
    "                next_Qvalues = torch.cat([next_Qvalues[:, 1:, :], torch.zeros((next_Qvalues.shape[0], 1, 1), device=next_Qvalues.device)], dim=1)\n",
    "                # shifted_pred_Qvalues = torch.cat([pred_Qvalues[:, 1:, :], torch.zeros((pred_Qvalues.shape[0], 1, 1), device=pred_Qvalues.device)], dim=1)\n",
    "                \n",
    "                TD_target = batch['context_rewards'] + gamma * next_Qvalues\n",
    "\n",
    "                loss = loss_fn(pred_Qvalues, TD_target)\n",
    "                test_loss += loss.item()\n",
    "        model.train()\n",
    "        test_loss /= len(test_dataloader)\n",
    "        # wandb logging\n",
    "        wandb.log({\"train_loss\": epoch_loss, \"test_loss\": test_loss})\n",
    "        print(f\"Epoch {epoch} | Train Loss {epoch_loss} | Test Loss {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [02:52<00:00, 14.51it/s]\n",
      "100%|██████████| 625/625 [00:31<00:00, 20.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 3.628277432346344 | Test Loss 3.6229422328948973\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [14:05<00:00,  2.96it/s]\n",
      "100%|██████████| 625/625 [02:58<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss 2.9377349327087403 | Test Loss 2.8170045455932615\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [18:53<00:00,  2.21it/s]\n",
      "100%|██████████| 625/625 [02:49<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss 2.812585419178009 | Test Loss 2.8255898761749267\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [19:59<00:00,  2.08it/s]\n",
      "100%|██████████| 625/625 [03:02<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss 2.8273637053489686 | Test Loss 2.843774157333374\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [18:18<00:00,  2.28it/s]\n",
      "100%|██████████| 625/625 [03:08<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss 2.843723430156708 | Test Loss 2.8676570240020753\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [18:43<00:00,  2.23it/s]\n",
      "100%|██████████| 625/625 [02:36<00:00,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss 2.867001423931122 | Test Loss 2.8928196712493897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Q_trainer(model, train_dataloader, test_dataloader, optimizer, loss_fn = loss_fn ,num_epochs=5, double=True, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), 'model/Qtransformer_model_random_0717_gamma_1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# randomChoose + Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load trajectory pkl\n",
    "traj_0 = pickle.load(open('data/RandomChoose_trajectories_part0.pkl', 'rb'))\n",
    "traj_1 = pickle.load(open('data/RandomChoose_trajectories_part1.pkl', 'rb'))\n",
    "traj_2 = pickle.load(open('data/RandomChoose_trajectories_part2.pkl', 'rb'))\n",
    "traj_3 = pickle.load(open('data/RandomChoose_trajectories_part3.pkl', 'rb'))\n",
    "traj_4 = pickle.load(open('data/RandomChoose_trajectories_part4.pkl', 'rb'))\n",
    "# integrate all trajectories\n",
    "traj = []\n",
    "traj.extend(traj_0)\n",
    "traj.extend(traj_1)\n",
    "traj.extend(traj_2)\n",
    "traj.extend(traj_3)\n",
    "traj.extend(traj_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrajectoryDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# batch_size = 32\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# create dataset\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTrajectoryDataset\u001b[49m(train_traj, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     12\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m TrajectoryDataset(test_traj, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# create dataloader\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TrajectoryDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# split trajectory into train and test\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_traj, test_traj = train_test_split(traj, test_size=0.2)\n",
    "\n",
    "batch_size = 16\n",
    "# batch_size = 32\n",
    "\n",
    "# create dataset\n",
    "train_dataset = TrajectoryDataset(train_traj, 10)\n",
    "test_dataset = TrajectoryDataset(test_traj, 10)\n",
    "\n",
    "# create dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# set up model\n",
    "# model = TransformerModel(embed_dim=70, num_heads=5)\n",
    "config = {\n",
    "        'horizon': 200,\n",
    "        'dim':5,\n",
    "        'act_num':10,\n",
    "        'state_dim': 50,\n",
    "        'action_dim': 10,\n",
    "        'n_layer': 8,\n",
    "        'n_embd': 32,\n",
    "        'n_head': 4,\n",
    "        'shuffle': True,\n",
    "        'dropout': 0.2,\n",
    "        'test': False,\n",
    "        'act_type': 'relu',\n",
    "        'pred_q': False\n",
    "        \n",
    "\n",
    "    }\n",
    "action_dim = config['action_dim']\n",
    "model = Transformer(config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device {device}\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:39<00:00, 22.73it/s]\n",
      "100%|██████████| 1250/1250 [00:38<00:00, 32.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss 7369.991474316406 | Test Loss 7369.60325234375\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:36<00:00, 23.05it/s]\n",
      "100%|██████████| 1250/1250 [00:36<00:00, 34.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss 7368.959986425782 | Test Loss 7369.124048828125\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:38<00:00, 22.84it/s]\n",
      "100%|██████████| 1250/1250 [00:35<00:00, 34.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss 7368.804815234375 | Test Loss 7368.98579140625\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:35<00:00, 23.24it/s]\n",
      "100%|██████████| 1250/1250 [00:31<00:00, 39.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss 7368.713823730469 | Test Loss 7368.627779296875\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:21<00:00, 24.78it/s]\n",
      "100%|██████████| 1250/1250 [00:32<00:00, 38.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss 7368.6082125 | Test Loss 7368.526080078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer(model, train_dataloader, test_dataloader, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), 'model/transformer_model_random_new.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinUCB + Qtransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trajectory pkl\n",
    "import pickle\n",
    "traj_0 = pickle.load(open('data/linucb_trajectories_part0.pkl', 'rb'))\n",
    "traj_1 = pickle.load(open('data/linucb_trajectories_part1.pkl', 'rb'))\n",
    "traj_2 = pickle.load(open('data/linucb_trajectories_part2.pkl', 'rb'))\n",
    "traj_3 = pickle.load(open('data/linucb_trajectories_part3.pkl', 'rb'))\n",
    "traj_4 = pickle.load(open('data/linucb_trajectories_part4.pkl', 'rb'))\n",
    "\n",
    "# integrate all trajectories\n",
    "traj = []\n",
    "traj.extend(traj_0)\n",
    "traj.extend(traj_1)\n",
    "traj.extend(traj_2)\n",
    "traj.extend(traj_3)\n",
    "traj.extend(traj_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\textgen\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin d:\\Anaconda3\\envs\\textgen\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    }
   ],
   "source": [
    "# split trajectory into train and test\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from icrl import *\n",
    "\n",
    "train_traj, test_traj = train_test_split(traj, test_size=0.2)\n",
    "\n",
    "batch_size = 16\n",
    "# batch_size = 64\n",
    "# batch_size = 32\n",
    "\n",
    "# create dataset\n",
    "train_dataset = TrajectoryDataset(train_traj, 10)\n",
    "test_dataset = TrajectoryDataset(test_traj, 10)\n",
    "\n",
    "# create dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxjxyys\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Lenovo/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\RA\\Tao Yao\\my_research\\ICRL\\In-context-RL\\wandb\\run-20240718_002352-xq5xyv5u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xjxyys/icrl_pretrain/runs/xq5xyv5u' target=\"_blank\">glamorous-leaf-49</a></strong> to <a href='https://wandb.ai/xjxyys/icrl_pretrain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xjxyys/icrl_pretrain' target=\"_blank\">https://wandb.ai/xjxyys/icrl_pretrain</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xjxyys/icrl_pretrain/runs/xq5xyv5u' target=\"_blank\">https://wandb.ai/xjxyys/icrl_pretrain/runs/xq5xyv5u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/xjxyys/icrl_pretrain/runs/xq5xyv5u?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x22392e7e980>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key = 'da9f2bbc59300d7434138182a83a9b35c0cea793') # login to wandb\n",
    "wandb.init(project=\"icrl_pretrain\", reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "config = {\n",
    "        'horizon': 200,\n",
    "        'dim':5,\n",
    "        'act_num':10,\n",
    "        'state_dim': 50,\n",
    "        'action_dim': 10,\n",
    "        'n_layer': 8,\n",
    "        'n_embd': 32,\n",
    "        # 'n_embd': 64,\n",
    "        'n_head': 4,\n",
    "        'shuffle': True,\n",
    "        'dropout': 0.2,\n",
    "        'test': False,\n",
    "        'act_type': 'relu',\n",
    "        'pred_q': True\n",
    "    }\n",
    "# train\n",
    "action_dim = config['action_dim']\n",
    "model = Transformer(config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5, weight_decay=1e-4)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "loss_fn = F.mse_loss\n",
    "gamma = 1\n",
    "# Trainer\n",
    "def Q_trainer(model, train_dataloader, test_dataloader, optimizer, num_epochs=10, double=False):\n",
    "    # record the loss before training\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(train_dataloader):\n",
    "            Qvalues = model(batch) # dimension: (batch_size, seq_len, A)\n",
    "            # pred_Qvalues = pred_Qvalues.gather(2, batch['context_actions'].unsqueeze(-1).long()) # get the Q value of the context actions\n",
    "            next_Qvalues = Qvalues.max(dim=-1, keepdim=True)[0]\n",
    "            next_Qvalues = torch.cat([next_Qvalues[:, 1:,:], torch.zeros((next_Qvalues.shape[0], 1, 1), device=next_Qvalues.device)], dim=1)\n",
    "            pred_Qvalues =  Qvalues * batch['context_actions']\n",
    "            pred_Qvalues =  pred_Qvalues.sum(dim=-1, keepdim=True)\n",
    "            TD_target = batch['context_rewards'] + gamma * next_Qvalues\n",
    "            loss = loss_fn(pred_Qvalues, TD_target)\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_dataloader)\n",
    "\n",
    "        for batch in tqdm.tqdm(test_dataloader):\n",
    "            # pred_Qvalues = model(batch).gather(2, batch['context_actions'].unsqueeze(-1).long())\n",
    "            Qvalues = model(batch)\n",
    "            next_Qvalues = Qvalues.max(dim=-1, keepdim=True)[0]\n",
    "            next_Qvalues = torch.cat([next_Qvalues[:, 1:, :], torch.zeros((next_Qvalues.shape[0], 1, 1), device=next_Qvalues.device)], dim=1)\n",
    "            pred_Qvalues =  Qvalues * batch['context_actions']\n",
    "            pred_Qvalues =  pred_Qvalues.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        \n",
    "            # shifted_pred_Qvalues = torch.cat([pred_Qvalues[:, 1:, :], torch.zeros((pred_Qvalues.shape[0], 1, 1), device=pred_Qvalues.device)], dim=1)\n",
    "            TD_target = batch['context_rewards'] + gamma * next_Qvalues\n",
    "            loss = loss_fn(pred_Qvalues, TD_target)\n",
    "            test_loss += loss.item()\n",
    "        test_loss /= len(test_dataloader)\n",
    "        print(f\"Train Loss {train_loss} | Test Loss {test_loss}\")\n",
    "        wandb.log({\"train_loss\": train_loss, \"test_loss\": test_loss})\n",
    "    # DataLoader can handle batching and shuffling\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    if double:\n",
    "        target_model = Transformer(config)\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "        target_model.eval()\n",
    "        target_model.to(device)\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        epoch_loss = 0.0\n",
    "        counter = 0\n",
    "        for batch in tqdm.tqdm(train_dataloader):\n",
    "            Qvalues = model(batch) # dimension: (batch_size, seq_len, A)\n",
    "            pred_Qvalues = Qvalues * batch['context_actions']\n",
    "            pred_Qvalues =  pred_Qvalues.sum(dim=-1, keepdim=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                next_actions = model(batch).max(dim=-1, keepdim=True)[1]\n",
    "\n",
    "                if double:\n",
    "                    # use target model to get the next Q values\n",
    "                    # next_Qvalues = target_model(batch).gather(2, next_actions.unsqueeze(-1).long())\n",
    "                    next_Qvalues = target_model(batch).gather(2, next_actions.long())\n",
    "                else:\n",
    "                    next_Qvalues = Qvalues.max(dim=-1, keepdim=True)[0]\n",
    "                next_Qvalues = torch.cat([next_Qvalues[:, 1:, :], torch.zeros((next_Qvalues.shape[0], 1, 1), device=next_Qvalues.device)], dim=1)\n",
    "            # Note: batch['context_rewards'] should be in the shape (batch_size, seq_len, 1) to match pred_Qvalues\n",
    "            TD_target = batch['context_rewards'] + gamma * next_Qvalues\n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(pred_Qvalues, TD_target, reduction='mean')\n",
    "            # Reset the gradients in the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # # Backward pass\n",
    "            loss.backward()\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            counter += 1\n",
    "            # if double:\n",
    "            #     for param, target_param in zip(model.parameters(), target_model.parameters()):\n",
    "            #         target_param.data = target_param.data * (1 - 0.01) + param.data * 0.01\n",
    "            if double:\n",
    "                if counter % 100 == 0:\n",
    "                    target_model.load_state_dict(model.state_dict())\n",
    "            \n",
    "        epoch_loss /= len(train_dataloader)\n",
    "        # Compute the test loss\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm.tqdm(test_dataloader):\n",
    "                # pred_Qvalues = model(batch).gather(2, batch['context_actions'].unsqueeze(-1).long())\n",
    "                Qvalues = model(batch)\n",
    "                pred_Qvalues = Qvalues * batch['context_actions']\n",
    "                pred_Qvalues =  pred_Qvalues.sum(dim=-1, keepdim=True)\n",
    "                next_actions = model(batch).max(dim=-1, keepdim=True)[1]\n",
    "                if double:\n",
    "    \n",
    "                    next_Qvalues = target_model(batch).gather(2, next_actions.long())\n",
    "                else:\n",
    "                    next_Qvalues = Qvalues.max(dim=-1, keepdim=True)[0]\n",
    "                    \n",
    "                next_Qvalues = torch.cat([next_Qvalues[:, 1:, :], torch.zeros((next_Qvalues.shape[0], 1, 1), device=next_Qvalues.device)], dim=1)\n",
    "                # shifted_pred_Qvalues = torch.cat([pred_Qvalues[:, 1:, :], torch.zeros((pred_Qvalues.shape[0], 1, 1), device=pred_Qvalues.device)], dim=1)\n",
    "                \n",
    "                TD_target = batch['context_rewards'] + gamma * next_Qvalues\n",
    "\n",
    "                loss = loss_fn(pred_Qvalues, TD_target)\n",
    "                test_loss += loss.item()\n",
    "        model.train()\n",
    "        test_loss /= len(test_dataloader)\n",
    "        # wandb logging\n",
    "        wandb.log({\"train_loss\": epoch_loss, \"test_loss\": test_loss})\n",
    "        print(f\"Epoch {epoch} | Train Loss {epoch_loss} | Test Loss {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:17<00:00, 25.25it/s]\n",
      "100%|██████████| 1250/1250 [00:48<00:00, 25.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 5.1115962317943575 | Test Loss 5.109612587547303\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [16:06<00:00,  5.17it/s]\n",
      "100%|██████████| 1250/1250 [02:04<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss 4.388273722934723 | Test Loss 4.728647190284729\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [18:33<00:00,  4.49it/s]\n",
      "100%|██████████| 1250/1250 [02:38<00:00,  7.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss 4.542934832811356 | Test Loss 5.000342235565186\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [19:05<00:00,  4.36it/s]\n",
      "100%|██████████| 1250/1250 [02:04<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss 3.8472036361694335 | Test Loss 3.8267644880294798\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [19:01<00:00,  4.38it/s]\n",
      "100%|██████████| 1250/1250 [02:37<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss 3.50114046959877 | Test Loss 3.638280979156494\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [19:49<00:00,  4.20it/s]\n",
      "100%|██████████| 1250/1250 [02:37<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss 3.373761801815033 | Test Loss 3.540104161262512\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [19:21<00:00,  4.30it/s]\n",
      "100%|██████████| 1250/1250 [02:07<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss 3.3358313569545746 | Test Loss 3.560681998825073\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [19:40<00:00,  4.24it/s]\n",
      "100%|██████████| 1250/1250 [02:08<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss 3.382911415100098 | Test Loss 3.5887659244537353\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [19:28<00:00,  4.28it/s]\n",
      "100%|██████████| 1250/1250 [02:11<00:00,  9.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss 3.6228999152183534 | Test Loss 4.011414376831055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Q_trainer(model, train_dataloader, test_dataloader, optimizer, num_epochs=8, double=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), 'model/Qtransformer_model_ucb_0718_gamma_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
