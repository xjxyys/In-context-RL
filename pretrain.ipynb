{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\textgen\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin d:\\Anaconda3\\envs\\textgen\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    }
   ],
   "source": [
    "from icrl import *\n",
    "from model import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trajectory pkl\n",
    "traj = pickle.load(open('data/linucb_trajectories.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:15ulejj8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>█▇▅▃▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▅▃▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>566.0982</td></tr><tr><td>train_loss</td><td>556.83414</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">volcanic-feather-13</strong> at: <a href='https://wandb.ai/xjxyys/icrl_pretrain/runs/15ulejj8' target=\"_blank\">https://wandb.ai/xjxyys/icrl_pretrain/runs/15ulejj8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240626_001454-15ulejj8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:15ulejj8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Lenovo\\Desktop\\RA\\Tao Yao\\my_research\\ICRL\\In-context-RL\\wandb\\run-20240626_085420-w8476mus</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xjxyys/icrl_pretrain/runs/w8476mus' target=\"_blank\">charmed-wildflower-14</a></strong> to <a href='https://wandb.ai/xjxyys/icrl_pretrain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xjxyys/icrl_pretrain' target=\"_blank\">https://wandb.ai/xjxyys/icrl_pretrain</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xjxyys/icrl_pretrain/runs/w8476mus' target=\"_blank\">https://wandb.ai/xjxyys/icrl_pretrain/runs/w8476mus</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/xjxyys/icrl_pretrain/runs/w8476mus?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1bacc68c160>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key = 'da9f2bbc59300d7434138182a83a9b35c0cea793') # login to wandb\n",
    "wandb.init(project=\"icrl_pretrain\", reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "# split trajectory into train and test\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_traj, test_traj = train_test_split(traj, test_size=0.2)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# create dataset\n",
    "train_dataset = TrajectoryDataset(train_traj, 10)\n",
    "test_dataset = TrajectoryDataset(test_traj, 10)\n",
    "\n",
    "# create dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# set up model\n",
    "# model = TransformerModel(embed_dim=70, num_heads=5)\n",
    "config = {\n",
    "        'horizon': 200,\n",
    "        'dim':5,\n",
    "        'act_num':10,\n",
    "        'state_dim': 50,\n",
    "        'action_dim': 10,\n",
    "        'n_layer': 8,\n",
    "        'n_embd': 32,\n",
    "        'n_head': 4,\n",
    "        'shuffle': True,\n",
    "        'dropout': 0.2,\n",
    "        'test': False,\n",
    "        'act_type': 'relu',\n",
    "\n",
    "    }\n",
    "action_dim = config['action_dim']\n",
    "model = Transformer(config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device {device}\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "# Trainer\n",
    "def trainer(model,train_dataloader, test_dataloader, optimizer, num_epochs=10):\n",
    "    # DataLoader can handle batching and shuffling\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        epoch_loss = 0.0\n",
    "        for batch in tqdm.tqdm(train_dataloader):\n",
    "            # Unpack the data\n",
    "            # tokens, action_labels = batch\n",
    "            # print(batch['context_actions'].size())\n",
    "            pred_actions = model(batch) # dimension: (batch_size, num_actions, seq_len)\n",
    "            # print(pred_actions.shape)\n",
    "            true_actions = batch['true_actions'] # dimension: (batch_size, seq_len)\n",
    "            # true_actions = true_actions.unsqueeze(-1).repeat(1, 1, action_dim) # dimension: (batch_size, seq_len, num_actions)\n",
    "            # print(true_actions)\n",
    "            # print(pred_actions)\n",
    "            # tokens  = tokens.to(device)\n",
    "            # action_labels = action_labels.to(device)\n",
    "            pred_actions_flat =  pred_actions.view(-1, action_dim)\n",
    "            true_actions_flat = true_actions.view(-1)\n",
    "            loss = loss_fn(pred_actions_flat, true_actions_flat)\n",
    "            # loss.backward()\n",
    "            # print(action_labels)\n",
    "            # Reset the gradients in the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # # type\n",
    "            # # Forward pass\n",
    "            # logits = model(tokens)  # Your forward method should handle the causal mask internally\n",
    "            \n",
    "            # # Compute loss\n",
    "            # loss = loss(logits, action_labels)\n",
    "    \n",
    "            # # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss /= len(train_dataloader)\n",
    "        # Compute the test loss\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm.tqdm(test_dataloader):\n",
    "                pred_actions = model(batch)\n",
    "                true_actions = batch['true_actions']\n",
    "                pred_actions_flat =  pred_actions.view(-1, action_dim)\n",
    "                true_actions_flat = true_actions.view(-1)\n",
    "                loss = loss_fn(pred_actions_flat, true_actions_flat)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "        model.train()\n",
    "        test_loss /= len(test_dataloader)\n",
    "        # wandb logging\n",
    "        wandb.log({\"train_loss\": epoch_loss, \"test_loss\": test_loss})\n",
    "        print(f\"Epoch {epoch} | Train Loss {epoch_loss} | Test Loss {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:38<00:00, 26.01it/s]\n",
      "100%|██████████| 250/250 [00:05<00:00, 48.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss 3240.8949246826173 | Test Loss 2563.71881640625\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:39<00:00, 25.53it/s]\n",
      "100%|██████████| 250/250 [00:04<00:00, 51.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss 2171.1034239501955 | Test Loss 1762.7617829589844\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:39<00:00, 25.15it/s]\n",
      "100%|██████████| 250/250 [00:04<00:00, 50.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss 1223.3654225463868 | Test Loss 941.5334782714843\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 24.07it/s]\n",
      "100%|██████████| 250/250 [00:05<00:00, 48.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss 831.0359356079101 | Test Loss 763.2508670654297\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 24.17it/s]\n",
      "100%|██████████| 250/250 [00:05<00:00, 46.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss 726.1762994689941 | Test Loss 924.7306782226563\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 24.21it/s]\n",
      "100%|██████████| 250/250 [00:05<00:00, 49.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss 641.9488928833008 | Test Loss 617.4172850341797\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 24.18it/s]\n",
      "100%|██████████| 250/250 [00:05<00:00, 49.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss 607.391990234375 | Test Loss 1016.5404844970703\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:40<00:00, 24.87it/s]\n",
      "100%|██████████| 250/250 [00:05<00:00, 49.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss 590.8535834350586 | Test Loss 603.2708479003907\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:40<00:00, 24.74it/s]\n",
      "100%|██████████| 250/250 [00:05<00:00, 49.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss 575.4490036010742 | Test Loss 577.7396866455078\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 23.97it/s]\n",
      "100%|██████████| 250/250 [00:05<00:00, 48.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss 582.6440943908691 | Test Loss 577.7813452148438\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:45<00:00, 21.92it/s]\n",
      "100%|██████████| 250/250 [00:06<00:00, 40.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss 557.7099358825684 | Test Loss 602.634320678711\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:44<00:00, 22.50it/s]\n",
      "100%|██████████| 250/250 [00:05<00:00, 42.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss 549.1451194763183 | Test Loss 567.8784359130859\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:40<00:00, 24.50it/s]\n",
      "100%|██████████| 250/250 [00:05<00:00, 47.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss 542.0763882446289 | Test Loss 588.7243215332031\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:40<00:00, 24.46it/s]\n",
      "100%|██████████| 250/250 [00:05<00:00, 48.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss 554.4992309875488 | Test Loss 544.5778474121093\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 24.39it/s]\n",
      "100%|██████████| 250/250 [00:04<00:00, 50.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss 529.9145792541503 | Test Loss 538.7370557861328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer(model, train_dataloader, test_dataloader, optimizer, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), 'model/transformer_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
