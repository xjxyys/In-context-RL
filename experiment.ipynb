{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icrl import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 10\n",
    "conext_dim = 5\n",
    "env = Environment(num_actions, conext_dim)\n",
    "\n",
    "# LinUCB\n",
    "linucb = LinUCB(num_actions, conext_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100,000 trajactories, 200 time steps each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 8344/100000 [17:10<3:15:52,  7.80it/s]"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "T = 200\n",
    "num_trajectories = 100000\n",
    "\n",
    "trajectories = [] # list of list of tuples [[(state,action,reward),...],...]\n",
    "# In this setting, s_t = \\mathbb{A} = action_set!\n",
    "all_regrets = np.zeros((num_trajectories, T))\n",
    "\n",
    "# use tqdm to show progress bar\n",
    "\n",
    "for i in tqdm.tqdm(range(num_trajectories)):\n",
    "    trajectory = []\n",
    "    total_regret = 0\n",
    "    regrets = []\n",
    "    best_action_index = env.get_best_action_index()  # Best action doesn't change in this setup\n",
    "    best_action_reward = np.dot(env.action_set[best_action_index], env.w_star)\n",
    "    for _ in range(T):\n",
    "        action_index = linucb.select_action(env.action_set)\n",
    "        reward, action = env.step(action_index)\n",
    "        # find action\n",
    "        linucb.update(action_index, reward, action)\n",
    "        \n",
    "        # Calculate regret for this round and add to total\n",
    "        round_regret = best_action_reward - reward\n",
    "        total_regret += round_regret\n",
    "        \n",
    "        trajectory.append((env.get_action_set(), action, reward))\n",
    "        regrets.append(total_regret)\n",
    "\n",
    "    all_regrets[i] = regrets # Store regrets for this trajectory\n",
    "    trajectories.append(trajectory) # Store trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dataframes to store the data\n",
    "import pandas as pd\n",
    "df_regrets= pd.DataFrame(all_regrets)\n",
    "df_regrets.to_csv('linucb_regrets.csv', index=False)\n",
    "\n",
    "\n",
    "# Save trajectories\n",
    "import pickle\n",
    "with open('linucb_trajectories.pkl', 'wb') as f:\n",
    "    pickle.dump(trajectories, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average regret and standard deviation\n",
    "average_regrets = np.mean(all_regrets, axis=0)\n",
    "std_regrets = np.std(all_regrets, axis=0)\n",
    "\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
